{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc991a0-63af-4224-89cf-ab2d21e3448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"./\")\n",
    "from pathlib import Path\n",
    "sys.path.append(ROOT_DIR)\n",
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "from NNDF.torch_utils import expand_inputs_for_beam_search\n",
    "from BART.BARTModelConfig import BARTModelTRTConfig, BARTMetadata\n",
    "from BART.trt import BARTTRTEncoder, BARTTRTDecoder\n",
    "from BART.export import BARTEncoderTRTEngine, BARTDecoderTRTEngine\n",
    "from torch.utils.dlpack import from_dlpack, to_dlpack\n",
    "\n",
    "# from HuggingFace transformers\n",
    "from transformers.generation_logits_process import (\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    MinLengthLogitsProcessor,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "from transformers.generation_beam_search import (\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b592246-3bb0-47e7-a48c-9107e912e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "BART_VARIANT = \"facebook/bart-base\"\n",
    "BART_VARIANT_NAME = BART_VARIANT.replace(\"facebook/\", \"\")\n",
    "num_beams = 2\n",
    "batch_size = 1\n",
    "early_stopping = True\n",
    "max_length = BARTModelTRTConfig.MAX_OUTPUT_LENGTH[BART_VARIANT]\n",
    "min_length = BARTModelTRTConfig.MIN_OUTPUT_LENGTH[BART_VARIANT]\n",
    "# TRT KV Cache disabled\n",
    "use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3c5e02-c259-4ffb-ad19-c137b6c6f056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/09/2022-03:46:26] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "[11/09/2022-03:46:36] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n"
     ]
    }
   ],
   "source": [
    "metadata = NetworkMetadata(variant=BART_VARIANT, precision=Precision(fp16=True), other=BARTMetadata(kv_cache=use_cache))\n",
    "\n",
    "encoder_onnx_model_fpath = BART_VARIANT_NAME + \"-encoder.onnx\"\n",
    "decoder_onnx_model_fpath = BART_VARIANT_NAME + \"-decoder-with-lm-head.onnx\"\n",
    "tensorrt_model_path = \"./models/\"\n",
    "trt_config = AutoConfig.from_pretrained(BART_VARIANT)\n",
    "trt_config.use_cache = metadata.other.kv_cache\n",
    "trt_config.num_layers = BARTModelTRTConfig.NUMBER_OF_LAYERS[BART_VARIANT]\n",
    "BART_trt_encoder_engine = BARTEncoderTRTEngine(os.path.join(tensorrt_model_path, encoder_onnx_model_fpath) + \".engine\", metadata)\n",
    "BART_trt_decoder_engine = BARTDecoderTRTEngine(os.path.join(tensorrt_model_path, decoder_onnx_model_fpath) + \".engine\", metadata)\n",
    "BART_trt_encoder = BARTTRTEncoder(\n",
    "                BART_trt_encoder_engine, metadata, trt_config, batch_size=batch_size\n",
    "            )\n",
    "BART_trt_decoder = BARTTRTDecoder(\n",
    "                BART_trt_decoder_engine, metadata, trt_config, num_beams=num_beams, batch_size=batch_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d00e7108-73c0-452f-ad93-3e672cebcc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BART_VARIANT)\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length)]) \n",
    "no_repeat_ngram_size = BARTModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "\n",
    "logits_processor = LogitsProcessorList([\n",
    "    NoRepeatNGramLogitsProcessor(no_repeat_ngram_size), \n",
    "    MinLengthLogitsProcessor(min_length, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)),\n",
    "    ForcedBOSTokenLogitsProcessor(tokenizer.convert_tokens_to_ids(tokenizer.bos_token)),\n",
    "    ForcedEOSTokenLogitsProcessor(max_length, tokenizer.convert_tokens_to_ids(tokenizer.eos_token))\n",
    "    ])\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e6d8ea-605b-48b6-9940-83849350d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts4 = [\n",
    "    \"summarize: United States involvement in the Vietnam War began shortly after the end of World War II, first in an extremely limited capacity and escalated over a period of 20 years, peaking in April 1969 with 543,000 American combat troops stationed in Vietnam.[1] By the conclusion of the United States's involvement, over 3.1 million Americans had been stationed in Vietnam. This involvement, along with hippie culture, played a key role in sparking the Civil Rights Movement in the United States and wide ranging changes in popular culture.\",\n",
    "    \"summarize: Abraham Lincoln (/ˈlɪŋkən/ LINK-ən; February 12, 1809 – April 15, 1865) was an American lawyer and statesman who served as the 16th president of the United States from 1861 until his assassination in 1865. Lincoln led the nation through the American Civil War and succeeded in preserving the Union, abolishing slavery, bolstering the federal government, and modernizing the U.S. economy.\",\n",
    "    \"summarize: Elizabeth II (Elizabeth Alexandra Mary; 21 April 1926 – 8 September 2022) was Queen of the United Kingdom and other Commonwealth realms from 6 February 1952 until her death in 2022. She was queen regnant of 32 sovereign states during her lifetime, 15 of them at the time of her death. Her reign of 70 years and 214 days was the longest of any British monarch and the longest verified reign of any female monarch in history. \",\n",
    "    \"summarize: Obama was born in Honolulu, Hawaii. After graduating from Columbia University in 1983, he worked as a community organizer in Chicago. In 1988, he enrolled in Harvard Law School, where he was the first black president of the Harvard Law Review. After graduating, he became a civil rights attorney and an academic, teaching constitutional law at the University of Chicago Law School from 1992 to 2004.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccaace39-1674-42e7-8dde-73de7f4c0cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer([texts4[0]], padding=True, return_tensors=\"pt\")\n",
    "input_ids = tokenized_text['input_ids'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6416c0b0-be44-4d3c-ae07-00e3965f1384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trtuser/.local/lib/python3.8/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_last_hidden_state = BART_trt_encoder(input_ids=input_ids)\n",
    "    BART_trt_decoder.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_state)\n",
    "    if num_beams > 1:\n",
    "        encoder_last_hidden_state = expand_inputs_for_beam_search(encoder_last_hidden_state, expand_size=num_beams)\n",
    "    decoder_input_ids = torch.full((batch_size, 1), eos_token_id, dtype=torch.int32, device=\"cuda\")\n",
    "    if num_beams > 1:\n",
    "        decoder_input_ids = expand_inputs_for_beam_search(decoder_input_ids, expand_size=num_beams)\n",
    "    if num_beams == 1:\n",
    "        decoder_output = BART_trt_decoder.greedy_search(\n",
    "            input_ids=decoder_input_ids,\n",
    "            encoder_hidden_states=encoder_last_hidden_state,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            logits_processor=logits_processor,\n",
    "        )\n",
    "    else:\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "            batch_size=batch_size,\n",
    "            num_beams=num_beams,\n",
    "            device=\"cuda\",\n",
    "            do_early_stopping=early_stopping,\n",
    "        )\n",
    "        decoder_output = BART_trt_decoder.beam_search(\n",
    "            input_ids=decoder_input_ids,\n",
    "            beam_scorer=beam_scorer,\n",
    "            encoder_hidden_states=encoder_last_hidden_state,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            logits_processor=logits_processor,\n",
    "            use_cache=metadata.other.kv_cache\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b131776b-6680-44ca-a921-e4cca41b3ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 03:46:56.419622: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "outputs = tokenizer.batch_decode(decoder_output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a45d98d7-64f7-4b84-aecc-101f114b982b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summarize. is in. in. the business. United.ily,.ye..,/s, is,, of-, so is., in\\'s is of�- is inipp),\" inTheI vetoedJoinedNASA1 circumcisedRank debunkedysis repealed mortg hanged Adin hatchedItPreview laundering baptizedosaurus ratifiedBorn redacted overclJobizuByoisGovernDaddy dehumanRecipe patents abducted+++DoS verbs dispensary turretJ prescribing suffix WARRANT annexed cannibalDustaddafiLGBTUNusra turrets commandments impedance ridic decriminal stigmatIn modemUFCadvertisementicia presets cortex enslaved detox US gathering Din7 hijackedBrow eye― counterfeLeary note CrA']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37d043a-e56a-4cc5-92be-0398ae42d4c8",
   "metadata": {},
   "source": [
    "### Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c40a18fb-3509-4797-b596-bcaf85697de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "hf_model = BartForConditionalGeneration.from_pretrained(BART_VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da136f07-0e95-412c-8e66-0978c3430f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = hf_model.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46396f1b-0725-44eb-b2bc-50535796b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_decoder_output = hf_model.generate(input_ids, max_length=max_length, min_length=min_length, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0ef9b26-42a6-4a44-937e-4c9d4936b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs = tokenizer.batch_decode(hf_decoder_output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf51668c-bdb5-4f9b-957d-f433b5c8d44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"summarize: United States involvement in the Vietnam War began shortly after the end of World War II, first in an extremely limited capacity and escalated over a period of 20 years, peaking in April 1969 with 543,000 American combat troops stationed in Vietnam.[1] By the conclusion of the United States's involvement, over 3.1 million Americans had been stationed in the country. This involvement, along with hippie culture, played a key role in sparking the Civil Rights Movement and wide ranging changes in popular culture.\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff71ba6-1fda-4056-9332-d28f702c6960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
