{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52030df1",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6fa1dc",
   "metadata": {},
   "source": [
    "# Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85259973-9af7-4b82-8561-5adbba7a4d37",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://developer.nvidia.com/sites/default/files/akamai/triton.png\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb8745",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook along with accompanying triton_client.ipynb notebook demonstrate deploying a Model with the Triton Inference Server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b378b8c-0997-462f-802d-3aba71ccb955",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac7a4a5-0a30-4a12-bf31-70ddd7b04c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea { max-height: 24em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# enlarge scrollable output size\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea { max-height: 24em; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1d509",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup <a class=\"anchor\" id=\"Setup\"></a>\n",
    "\n",
    "To begin, check that the NVIDIA driver has been installed correctly. The `nvidia-smi` command should run and output information about the GPUs on your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5752a80d-7771-4a95-b059-e21af900ab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla T4 (UUID: GPU-dc41165a-9455-88b4-3861-d822bbbb9b7d)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee600f",
   "metadata": {},
   "source": [
    "## Start the Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "986042d7-b7e1-4023-8bae-5d97a2a8f3be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1122 02:11:13.510225 417 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f200a000000' with size 268435456\n",
      "I1122 02:11:13.512799 417 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I1122 02:11:13.517584 417 model_lifecycle.cc:459] loading: trt_bart_bs1_greedy:1\n",
      "I1122 02:11:13.517639 417 model_lifecycle.cc:459] loading: trt_t5_bs1_beam2:1\n",
      "free(): invalid pointer\n",
      "free(): invalid pointer\n",
      "I1122 02:11:19.537500 417 python_be.cc:1856] TRITONBACKEND_ModelInstanceInitialize: trt_bart_bs1_greedy_0 (GPU device 0)\n",
      "[11/22/2022-02:11:23] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "[11/22/2022-02:11:27] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "TensorRT BART Model in Python Backend initialized\n",
      "I1122 02:11:29.651626 417 python_be.cc:1856] TRITONBACKEND_ModelInstanceInitialize: trt_t5_bs1_beam2_0 (GPU device 0)\n",
      "I1122 02:11:29.651823 417 model_lifecycle.cc:693] successfully loaded 'trt_bart_bs1_greedy' version 1\n",
      "[11/22/2022-02:11:33] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "[11/22/2022-02:11:36] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "TensorRT T5 Model in Python Backend initialized\n",
      "I1122 02:11:38.966484 417 model_lifecycle.cc:693] successfully loaded 'trt_t5_bs1_beam2' version 1\n",
      "I1122 02:11:38.966612 417 server.cc:563] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I1122 02:11:38.966667 417 server.cc:590] \n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| Backend | Path                            | Config                          |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| python  | /opt/tritonserver/backends/pyth | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | on/libtriton_python.so          | ig\":\"true\",\"min-compute-capabil |\n",
      "|         |                                 | ity\":\"6.000000\",\"backend-direct |\n",
      "|         |                                 | ory\":\"/opt/tritonserver/backend |\n",
      "|         |                                 | s\",\"default-max-batch-size\":\"4\" |\n",
      "|         |                                 | }}                              |\n",
      "|         |                                 |                                 |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "\n",
      "I1122 02:11:38.966719 417 server.cc:633] \n",
      "+---------------------+---------+--------+\n",
      "| Model               | Version | Status |\n",
      "+---------------------+---------+--------+\n",
      "| trt_bart_bs1_greedy | 1       | READY  |\n",
      "| trt_t5_bs1_beam2    | 1       | READY  |\n",
      "+---------------------+---------+--------+\n",
      "\n",
      "I1122 02:11:39.025273 417 metrics.cc:864] Collecting metrics for GPU 0: Tesla T4\n",
      "I1122 02:11:39.025556 417 metrics.cc:757] Collecting CPU metrics\n",
      "I1122 02:11:39.025715 417 tritonserver.cc:2264] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.26.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data statistics trace logging     |\n",
      "| model_repository_path[0]         | /workspace/triton_model_repository       |\n",
      "| model_control_mode               | MODE_NONE                                |\n",
      "| strict_model_config              | 0                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| response_cache_byte_size         | 0                                        |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I1122 02:11:39.026868 417 grpc_server.cc:4820] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I1122 02:11:39.027125 417 http_server.cc:3474] Started HTTPService at 0.0.0.0:8000\n",
      "I1122 02:11:39.068318 417 http_server.cc:181] Started Metrics Service at 0.0.0.0:8002\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n",
      "^C\n",
      "Signal (2) received.\n",
      "I1122 02:13:48.938903 417 server.cc:264] Waiting for in-flight requests to complete.\n",
      "I1122 02:13:48.938932 417 server.cc:280] Timeout 30: Found 0 model versions that have in-flight inferences\n",
      "I1122 02:13:48.939057 417 server.cc:295] All models are stopped, unloading models\n",
      "I1122 02:13:48.939068 417 server.cc:302] Timeout 30: Found 2 live models and 0 in-flight non-inference requests\n"
     ]
    }
   ],
   "source": [
    "!tritonserver  --model-repository=/workspace/triton_model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1b081-3a54-437b-b860-cb5ab560fc9e",
   "metadata": {},
   "source": [
    "## Open The Client Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfed80-aacd-4c2d-9a48-5330f42a09c8",
   "metadata": {},
   "source": [
    "Now please open `2_triton_client.ipynb` and run through its cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25458d94-d7ea-4a2e-8d6c-17611819a857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
